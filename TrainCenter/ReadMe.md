## 版本号：
- dev2: 采用从后向前抽取数据到经验回放池的方案进行神经网络的训练学习，

0 更新内容:
    0.1: 完善与其他AI引擎博弈的接口，可选择使用GUI、终端Terminal进行手动博弈或使用接口进行自动博弈
    0.2: 将UCT中的超参数Cpuct修改为动态调整，具体做法是将Cpuct正比于每步的策略损失(loss_step_p)
    0.3: 求UCT时为了避免过度拟合，增加一定的概率使用均匀概率替换网络预测策略p进行计算


 1 蒙特卡洛搜索树:
    1.1: 每步清空MCTS树;
    1.2: 使用温度参数T对实际下棋时的策略P做修正;
    1.3: 每次从整个MCTS树中选取符合条件(N、Q)的数据训练;
    1.4: 每个结点的信息(N, Q, Pi, Child)由其父亲存储;

 2 经验回放池数据处理:
    
    2.1: 同次迭代内数据融合: 存在相同棋盘时选取模拟次数(N)多的数据p,v作为标签;
    2.2: 新旧数据融合: 新数据中不存在新棋盘则直接加入, 存在则对新旧数据根据迭代差做折扣后融合;
    2.3: 使用 N + |Q| 作为入选经验回放池数据的阈值指标, 实现自动从下至上训练数据;

 3 多进程:

    3.1: 将各自进程的数据存储至磁盘后，主进程读取融合后放入经验回放池中训练;

 4 数据记录:

    4.1: 记录每次迭代生成数据, 训练和总时间，存储在time_train.txt文件中;
    4.2: 记录自博弈对弈情况, 存储在 play_record.txt中;
    4.3: 记录经验回访池中每一步的数据量; 存储在step_data_num.txt中;
    4.4: 记录MCTS树中每一步结点的网络预测p(先验概率)与模拟得到的策略(后验概率)之间的交叉熵、价值的均方差损失; 存储在loss_step_p.txt和loss_step_v.txt中（预期趋势：越接近根结点loss越大，越接近叶子结点越小）
    4.5: 记录训练损失，存储在loss.txt中
    4.6: 记录棋盘棋谱，存储在play_record/文件夹中
    4.7: 记录控制栏输出日志，存储在log_main_process.log中

 5 需要做的:
    
    5.7: 观察是否存在大棋盘步数较多时Q值反向传播存在信号逐渐变弱的情况。(Q值不取所有的平均)



 6 运行环境:

    6.1: python   v3.8.10
    
    6.2: numpy    v1.19.1
    
    6.3: pytorch  v1.6.0
    
    6.4: pathos   v0.2.6
    
    6.5: pyqt     v5.9.7

 7 实验心得:
    
    7.1: 提取数据时根据P提取相当于从顶向下学(根据不可信推可信)、根据Q提取相当于从底向上学习(根据可信推不可信) ----后者更好

 8 新想法:

    8.1: 能不能不限制提取数据，但限制不同layers数据进行网络训练。具体说是我之前根据Q+N作为数据是否放入经验回放池中的条件，
         现在我直接将所有数据放入到replaybuffer中，然后根据loss_step_p各步骤收敛情况选取急需训练的数据以较大概率被选中进行训练，
         其它数据给以较小的概率被选中。（可能效果不好）
    8.2: 使用迭代思想，设置固定的训练滑动窗口，只采集指定窗口内满足Q、N条件的数据进行训练；当窗口从后滑动到初始局面时再继续从最后向前滑动，
         迭代进行。窗口滑动的训练间隔可以人为设置，也可以根据loss设置。


​         
 9 新想法（2021.07.04）

    9.1: Mini-max归一化了Q,使之范围是[0,1];
            ？？？？？（1）采用归一化以后的数值，作为值损失函数的目标值。
            ？？？？？（2）采用加权的方法，突出前几步，损失值小，但样本却非常重要，应在损失函数中调高其权重。
    9.2: 新增加log(sigma{P(s,a)|a is an legal action})损失;
    9.3: 归一化P(s,a),使得sigma{P(s,a)|a is a legal action})=1;
    9.4: 不用于学习的样本：跳过U(s,a)主导的样本，跳过随机的样本；
    9.5: 用采样频率对样本进行加权；
    9.6: 修改样本入选训练集的条件；
    9.7: 用AutoEncoder约束隐藏层编码，新增重构误差；
    9.8: 新增能量损失，用于判别该状态出现的概率；
    9.9: 可视化；
    9.10: 进一步做好统计。
    9.11: 将旧数据抛掉，目前仅缓存4~5代数据，更旧的直接抛弃掉。否则训练时间过长，得不偿失。
    9.12【方向】: 还应该保持从后向前学习，目前训练时数据的质量不佳。
    9.13: 候选走法hash表，单独成一个文件，保存到磁盘上。
    9.14: 改进hash表，目前检索、存储、修改太慢 。增量的hash键值。
    
    注意：旧方法，从后向前训练，训练集保存了完美的知识，因此，数据可以长期保存，从后向前训练是不是复习即可；
         新方法，并未体现从后向前，目前，残局走的不稳，经验回放池中保留旧数据的代数又太多，总体效果似乎在变差。
         
    超参数集合；
    约束集合的思想。并且，把约束明确写入损失函数。
    
    5x5棋盘上，训练比较稳定。

# 优化现有的结构
    1.可变目标，优化现有的loss结构。
        a.loss里面加 剩余棋子数量，剩余king的数量。（加神经网络也可以尝试一下）
        b.channel加一个，修改原来的思路为动态变化的棋子重要性（估值）。
        c.本方的king，本方的棋子数目，对方的king，对方的棋子数目。
    2.History信息加channel送入神经网络
    3.