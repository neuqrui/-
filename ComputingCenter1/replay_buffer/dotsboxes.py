from collections import namedtuple
import numpy as np
import torch as t
import gc

total_tuple = namedtuple('total_tuple', ['num_iter', 'layers', 'visit_num', 'data'])
iter_tuple = namedtuple('iter_tuple', ['num_iter', 'layers', 'visit_num', 'data'])


class ReplayBuffer:

    def __init__(self, args):
        self.args = args
        self.total_data = dict()
        self.iter_data = dict()
        self.trajectory = []
        # Use the list to record the amount of data at each step
        self.num_step, self.boards, self.target_policy, self.num_iter, self.target_values, self.num_lay = None, None, None, None, None, None
        self.sample_ids = [np.array([]) for _ in range(self.args.num_net)]

    def get_step_data_num(self):
        return self.num_step

    def get_total_data_num(self):
        return int(np.sum(self.num_step)/self.args.num_net)

    def get_iter_data_num(self):
        return len(self.iter_data.keys())

    def get_batch(self):
        """
        Get training data from experience replay pool
        :returns return_b:(torch.tensor) board data
                 return_p:(torch.tensor) pi data
                 return_q:(torch.tensor) q data
        """
        return_b,  return_p,  return_q, return_iter, return_lay = [t.tensor(np.array([]).astype(np.float64)) for _ in range(self.args.num_net)], \
                                                                  [t.tensor(np.array([]).astype(np.float64)) for _ in range(self.args.num_net)], \
                                                                  [t.tensor(np.array([]).astype(np.float64)) for _ in range(self.args.num_net)], \
                                                                  [t.tensor(np.array([]).astype(np.float64)) for _ in range(self.args.num_net)], \
                                                                  [t.tensor(np.array([]).astype(np.float64)) for _ in range(self.args.num_net)]
        for i in range(self.args.num_net):
            self.sample_ids[i] = np.array([])
            self.sample_ids[i] = np.random.randint(len(self.boards[i]), size=self.args.batch_size)
            return_b[i] = t.tensor(np.array([self.boards[i][j] for j in self.sample_ids[i]]).astype(np.float64)).float()
            return_p[i] = t.tensor(np.array([self.target_policy[i][j] for j in self.sample_ids[i]]).astype(np.float64)).float()
            return_q[i] = t.tensor(np.array([self.target_values[i][j] for j in self.sample_ids[i]]).astype(np.float64)).float()
            return_iter[i] = t.tensor(np.array([self.num_iter[i][j] for j in self.sample_ids[i]]).astype(np.float64)).float()
            return_lay[i] = t.tensor(np.array([self.num_lay[i][j] for j in self.sample_ids[i]]).astype(np.float64)).float()

        return return_b, return_p, return_q, return_iter, return_lay

    def add_one_piece_data(self, num_iter, layers, visit_num, data):
        """
        Store the data of an action in memory
        :param num_iter:(int) number of iterations
        :param layers:(int) chessboard moves step
        :param visit_num:(int) the number of the own node has been visited
        :param data:(list) [board, pi, v] the posterior policy and value of each node in the tree
        """
        key = data[0].tostring()
        if key not in self.iter_data:
            # data is not in dict
            self.iter_data[key] = iter_tuple._make([num_iter, layers, visit_num, data])
        else:
            # combine data according to conditions
            if visit_num >= self.iter_data[key].visit_num:
                self.iter_data[key] = iter_tuple._make([num_iter, layers, visit_num, data])

    def add_trajectory(self, layers: int, board, net_pi, pi, net_v, v):
        """
        Store the information of each step
        :param layers:(int) the depth of tree
        :param net_pi:(list) prior policy of network output
        :param pi:(list) simulated posteriori policy
        :param net_v:(float) the prior value of network output
        :param v:(float) posterior value after simulation
        """
        self.trajectory.append([layers, board, net_pi, pi, net_v, v])

    def merge_data(self, num_iter):
        """
        Combine the data generated by each iteration into the total data
        :param num_iter:(int) :number of iterations
        """
        # print("1" * 50)
        # Using three lists to record three types different data to divide them into three different network learning
        self.num_iter, self.boards, self.target_policy, self.target_values, self.num_lay = [[] for _ in range(self.args.num_net)], \
                                                                                           [[] for _ in range(self.args.num_net)], \
                                                                                           [[] for _ in range(self.args.num_net)], \
                                                                                           [[] for _ in range(self.args.num_net)], \
                                                                                           [[] for _ in range(self.args.num_net)]
        # print("2" * 50)
        # Use the list to record the amount of data at each step
        self.num_step = np.zeros(self.args.num_max_layers, dtype=np.int32)
        # print("3" * 50)
        # Add new data to the replay pool
        for key, (iter_num, layers, visit_num, data) in self.iter_data.items():
            # assert num_iter == iter_num
            self.total_data[key] = total_tuple._make([iter_num, layers, visit_num, data])
        # print("4" * 50)
        # Delete early data and integrate data
        for key in list(self.total_data.keys()):
            if num_iter - self.total_data[key].num_iter > self.args.replay_buffer_threshold:
                del self.total_data[key]
            else:
                # Classified storage data
                self.num_step[self.total_data[key].layers] += 1
                self.boards[self.total_data[key].layers % self.args.num_net].append(self.total_data[key].data[0])
                self.target_policy[self.total_data[key].layers % self.args.num_net].append(self.total_data[key].data[1])
                self.target_values[self.total_data[key].layers % self.args.num_net].append(self.total_data[key].data[2])
                self.num_iter[self.total_data[key].layers % self.args.num_net].append(self.total_data[key].num_iter)
                self.num_lay[self.total_data[key].layers % self.args.num_net].append(self.total_data[key].layers)
        # print("5" * 50)
        # Clear the current iteration data
        self.iter_data = dict()
        print("Training data merge completed.")
        gc.collect()